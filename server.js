// server.js - DeepSeek v3.2 mit Auto-Continuation fÃ¼r lange Antworten
const express = require('express');
const cors = require('cors');
const axios = require('axios');

const app = express();
const PORT = process.env.PORT || 3000;

app.use(cors());
app.use(express.json());

const NIM_API_BASE = process.env.NIM_API_BASE || 'https://integrate.api.nvidia.com/v1';
const NIM_API_KEY = process.env.NIM_API_KEY;

const SHOW_REASONING = false;
const ENABLE_THINKING_MODE = false;

// ğŸ”¥ AUTO-CONTINUATION SETTINGS
const ENABLE_AUTO_CONTINUATION = true;   // â† Automatisch verlÃ¤ngern
const MIN_DESIRED_TOKENS = 2000;         // â† Minimum: ~1000 WÃ¶rter
const MAX_CONTINUATIONS = 2;             // â† Max 3x weiterschreiben lassen

const STRUCTURED_PROMPT = `Du bist ein immersiver, ausfÃ¼hrlicher Rollenspiel-Partner und GeschichtenerzÃ¤hler.

SCHREIBSTIL:
- Nutze beschreibende, bildhafte Sprache
- Beschreibe Szenen mit allen Sinnen (sehen, hÃ¶ren, riechen, fÃ¼hlen, schmecken)
- Zeige Emotionen und innere Gedanken der Charaktere
- Erschaffe eine immersive AtmosphÃ¤re
- Bleibe im Charakter

STRUKTUR (fÃ¼r jede Antwort):
1. Beschreibe die Szene/Umgebung im Detail
2. Beschreibe Charaktere, ihre KÃ¶rpersprache und Emotionen
3. Beschreibe die Handlung und Interaktionen ausfÃ¼hrlich
4. FÃ¼ge sensorische Details hinzu (GerÃ¼che, GerÃ¤usche, Texturen)
5. Entwickle die Situation weiter oder stelle eine Frage

Sei ausfÃ¼hrlich und detailliert. QualitÃ¤t Ã¼ber KÃ¼rze.`;

const MODEL_CONFIG = {
  'deepseek-ultra': {
    model: 'deepseek-ai/deepseek-v3.2',
    systemPrompt: STRUCTURED_PROMPT,
    temperature: 0.85,
    max_tokens: 6000,
    top_p: 0.92,
    frequency_penalty: 0.5,
    presence_penalty: 0.7
  },
  'gpt-4o': {
    model: 'deepseek-ai/deepseek-v3.2',
    systemPrompt: STRUCTURED_PROMPT,
    temperature: 0.85,
    max_tokens: 6000,
    top_p: 0.92,
    frequency_penalty: 0.5,
    presence_penalty: 0.7
  },
  'gpt-4': {
    model: 'deepseek-ai/deepseek-v3.2',
    systemPrompt: STRUCTURED_PROMPT,
    temperature: 0.8,
    max_tokens: 5000,
    top_p: 0.9,
    frequency_penalty: 0.4,
    presence_penalty: 0.6
  },
  'gpt-3.5-turbo': {
    model: 'deepseek-ai/deepseek-v3.2',
    systemPrompt: STRUCTURED_PROMPT,
    temperature: 0.8,
    max_tokens: 5000,
    top_p: 0.9,
    frequency_penalty: 0.4,
    presence_penalty: 0.6
  }
};

// Continuation Prompts - verschiedene Varianten fÃ¼r Abwechslung
const CONTINUATION_PROMPTS = [
  "Bitte fahre mit der Beschreibung fort und fÃ¼ge weitere Details hinzu.",
  "Entwickle die Szene weiter und beschreibe, was als nÃ¤chstes passiert.",
  "FÃ¼ge mehr Details zur AtmosphÃ¤re und den Charakteren hinzu.",
  "Beschreibe die Situation noch ausfÃ¼hrlicher mit mehr sensorischen Details.",
  "Setze die Beschreibung fort und entwickle die Handlung weiter."
];

app.get('/health', (req, res) => {
  res.json({ 
    status: 'ok', 
    service: 'DeepSeek v3.2 Auto-Continuation Proxy',
    features: {
      auto_continuation: ENABLE_AUTO_CONTINUATION,
      min_desired_tokens: MIN_DESIRED_TOKENS,
      max_continuations: MAX_CONTINUATIONS
    }
  });
});

app.get('/v1/models', (req, res) => {
  const models = Object.keys(MODEL_CONFIG).map(model => ({
    id: model,
    object: 'model',
    created: Date.now(),
    owned_by: 'nvidia-nim-proxy'
  }));
  
  res.json({
    object: 'list',
    data: models
  });
});

async function makeAPICall(config) {
  const response = await axios.post(`${NIM_API_BASE}/chat/completions`, config, {
    headers: {
      'Authorization': `Bearer ${NIM_API_KEY}`,
      'Content-Type': 'application/json'
    },
    timeout: 120000
  });
  return response.data;
}

async function getContinuedResponse(initialMessages, config, initialContent) {
  let fullContent = initialContent;
  let totalTokens = config.max_tokens || 0;
  let continuations = 0;
  
  console.log(`ğŸ”„ Starting continuation... Initial tokens: ~${Math.round(initialContent.length / 4)}`);
  
  while (continuations < MAX_CONTINUATIONS) {
    const currentTokens = Math.round(fullContent.length / 4);
    
    if (currentTokens >= MIN_DESIRED_TOKENS) {
      console.log(`âœ… Target reached: ${currentTokens} tokens (~${Math.round(currentTokens * 0.75)} words)`);
      break;
    }
    
    continuations++;
    const continuationPrompt = CONTINUATION_PROMPTS[continuations % CONTINUATION_PROMPTS.length];
    
    console.log(`ğŸ”„ Continuation ${continuations}/${MAX_CONTINUATIONS}: "${continuationPrompt}"`);
    
    // Erstelle neue Messages mit bisheriger Antwort + Continuation Request
    const continuationMessages = [
      ...initialMessages,
      { role: 'assistant', content: fullContent },
      { role: 'user', content: continuationPrompt }
    ];
    
    const continuationConfig = {
      ...config,
      messages: continuationMessages
    };
    
    try {
      const response = await makeAPICall(continuationConfig);
      const newContent = response.choices[0]?.message?.content || '';
      
      if (newContent.length < 50) {
        console.log(`âš ï¸ Continuation too short, stopping`);
        break;
      }
      
      // FÃ¼ge neue Inhalte hinzu (mit Absatz-Trennung)
      fullContent += '\n\n' + newContent;
      
      console.log(`ğŸ“Š After continuation ${continuations}: ~${Math.round(fullContent.length / 4)} tokens`);
      
    } catch (error) {
      console.error(`âŒ Continuation ${continuations} failed:`, error.message);
      break;
    }
  }
  
  return fullContent;
}

app.post('/v1/chat/completions', async (req, res) => {
  try {
    const { model, messages, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, stream } = req.body;
    
    console.log(`ğŸ“¨ Request: model=${model}, stream=${stream || false}`);
    
    let config = MODEL_CONFIG[model];
    
    if (!config) {
      config = {
        model: 'deepseek-ai/deepseek-v3.2',
        systemPrompt: STRUCTURED_PROMPT,
        temperature: 0.85,
        max_tokens: 6000,
        top_p: 0.92,
        frequency_penalty: 0.5,
        presence_penalty: 0.7
      };
    }
    
    // System Prompt hinzufÃ¼gen
    let processedMessages = [...messages];
    if (config.systemPrompt && !messages.some(m => m.role === 'system')) {
      processedMessages.unshift({
        role: 'system',
        content: config.systemPrompt
      });
    }
    
    const finalConfig = {
      model: config.model,
      messages: processedMessages,
      temperature: temperature !== undefined ? temperature : config.temperature,
      max_tokens: max_tokens !== undefined ? max_tokens : config.max_tokens,
      top_p: top_p !== undefined ? top_p : config.top_p,
      frequency_penalty: frequency_penalty !== undefined ? frequency_penalty : config.frequency_penalty,
      presence_penalty: presence_penalty !== undefined ? presence_penalty : config.presence_penalty,
      stream: false  // Continuation funktioniert nur non-stream
    };
    
    console.log(`âœ… Sending to NVIDIA: max_tokens=${finalConfig.max_tokens}, model=${finalConfig.model}`);
    
    // Erste Antwort holen
    const initialResponse = await makeAPICall(finalConfig);
    let finalContent = initialResponse.choices[0]?.message?.content || '';
    
    const initialTokens = Math.round(finalContent.length / 4);
    console.log(`ğŸ“Š Initial response: ${initialTokens} tokens (~${Math.round(initialTokens * 0.75)} words)`);
    
    // Auto-Continuation wenn aktiviert und zu kurz
    if (ENABLE_AUTO_CONTINUATION && !stream && initialTokens < MIN_DESIRED_TOKENS) {
      console.log(`âš ï¸ Response too short (${initialTokens} < ${MIN_DESIRED_TOKENS}), starting continuation...`);
      finalContent = await getContinuedResponse(processedMessages, finalConfig, finalContent);
    }
    
    // Streaming-Response (wenn ursprÃ¼nglich requested)
    if (stream) {
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');
      
      // Simuliere Streaming indem wir Wort fÃ¼r Wort senden
      const words = finalContent.split(' ');
      for (let i = 0; i < words.length; i++) {
        const chunk = {
          id: `chatcmpl-${Date.now()}`,
          object: 'chat.completion.chunk',
          created: Math.floor(Date.now() / 1000),
          model: model,
          choices: [{
            index: 0,
            delta: {
              content: (i === 0 ? '' : ' ') + words[i]
            },
            finish_reason: i === words.length - 1 ? 'stop' : null
          }]
        };
        
        res.write(`data: ${JSON.stringify(chunk)}\n\n`);
        
        // Kleine VerzÃ¶gerung fÃ¼r natÃ¼rliches Streaming
        if (i % 10 === 0) {
          await new Promise(resolve => setTimeout(resolve, 50));
        }
      }
      
      res.write('data: [DONE]\n\n');
      res.end();
      
    } else {
      // Non-streaming response
      const finalTokens = Math.round(finalContent.length / 4);
      const wordCount = Math.round(finalTokens * 0.75);
      
      const openaiResponse = {
        id: `chatcmpl-${Date.now()}`,
        object: 'chat.completion',
        created: Math.floor(Date.now() / 1000),
        model: model,
        choices: [{
          index: 0,
          message: {
            role: 'assistant',
            content: finalContent
          },
          finish_reason: 'stop'
        }],
        usage: {
          prompt_tokens: Math.round(processedMessages.reduce((acc, m) => acc + m.content.length / 4, 0)),
          completion_tokens: finalTokens,
          total_tokens: finalTokens + Math.round(processedMessages.reduce((acc, m) => acc + m.content.length / 4, 0))
        }
      };
      
      console.log(`âœ… Final response: ${finalTokens} tokens (~${wordCount} words)`);
      
      res.json(openaiResponse);
    }
    
  } catch (error) {
    console.error('âŒ Proxy error:', error.response?.data || error.message);
    
    res.status(error.response?.status || 500).json({
      error: {
        message: error.response?.data?.error?.message || error.message || 'Internal server error',
        type: 'invalid_request_error',
        code: error.response?.status || 500
      }
    });
  }
});

app.all('*', (req, res) => {
  res.status(404).json({
    error: {
      message: `Endpoint ${req.path} not found`,
      type: 'invalid_request_error',
      code: 404
    }
  });
});

app.listen(PORT, () => {
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸš€ DeepSeek v3.2 Auto-Continuation Proxy');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log(`ğŸ“¡ Port: ${PORT}`);
  console.log(`ğŸ”„ Auto-continuation: ${ENABLE_AUTO_CONTINUATION ? 'ENABLED âœ…' : 'DISABLED'}`);
  console.log(`ğŸ“Š Target: ${MIN_DESIRED_TOKENS} tokens (~${Math.round(MIN_DESIRED_TOKENS * 0.75)} words)`);
  console.log(`ğŸ” Max continuations: ${MAX_CONTINUATIONS}`);
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
});
